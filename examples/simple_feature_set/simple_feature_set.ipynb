{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://66.media.tumblr.com/tumblr_macyx8VqU11rfjowdo1_500.gif)\n",
    "\n",
    "\n",
    "# #1 Discovering Butterfree - Feature Set Basics\n",
    "\n",
    "Welcome to **Discovering Butterfree** tutorial series!\n",
    "\n",
    "This first tutorial will cover some basics of Butterfree library and you learn how to create your first feature set :rocket: :rocket:\n",
    "\n",
    "Before diving into the tutorial make sure you have a basic understanding of these main data concepts: **features**, **feature sets** and the **\"Feature Store Architecture\"**, you can read more about this [here]().\n",
    "\n",
    "## Library Basics:\n",
    "\n",
    "Buterfree's main objective is to make feature engineering easy. The library provides a high-level API for declarative feature definitions. But behind these abstractions, Butterfree is essentially an **ETL (Extract - Transform - Load)** framework, so this reflects in terms of the organization of the project.\n",
    "\n",
    "### Extract\n",
    "\n",
    "`from butterfree.core.extract import ...`\n",
    "\n",
    "Module with the entities responsible for extracting data into the pipeline. The module provides the following tools:\n",
    "\n",
    "* `readers`: data connectors. Currently Butterfree provides readers for files, tables registered in Spark Hive metastore, and Kafka topics.\n",
    "\n",
    "\n",
    "* `pre_processing`: a utility tool for making some transformations or re-arrange the structure of the reader's input data before the feature engineering.\n",
    "\n",
    "\n",
    "* `source`: a composition of `readers`. The entity responsible for merging datasets coming from the defined readers into a single dataframe input for the `Transform` stage.\n",
    "\n",
    "### Transform\n",
    "\n",
    "`from butterfree.core.transform import ...`\n",
    "\n",
    "The main module of the library, responsible for feature engineering, in other words, all the transformations on the data. The module provides the following main tools:\n",
    "\n",
    "* `features`: the entity that defines what a feature is. Holds a transformation and metadata about the feature.\n",
    "\n",
    "\n",
    "* `transformations`: provides a set of components for transforming the data, with the possibility to use Spark native functions, aggregations, SQL expressions and others. \n",
    "\n",
    "\n",
    "* `feature_set`: an entity that defines a feature set. Holds features and the metadata around it.\n",
    "\n",
    "\n",
    "### Load\n",
    "\n",
    "`from butterfree.core.load import ...`\n",
    "\n",
    "The module is responsible for saving the data in some data storage. The module provides the following tools:\n",
    "\n",
    "* `writers`: provide connections to data sources to write data. Currently Butterfree provides ways to save data on S3 registered as tables Spark Hive metastore and to Cassandra DB.\n",
    "\n",
    "\n",
    "* `sink`: a composition of writers. The entity responsible for triggering the writing jobs on a set of defined writers\n",
    "\n",
    "### Pipelines\n",
    "\n",
    "Pipelines are responsible for integrating all other modules (`extract`, `transform`, `load`) in order to define complete ETL jobs from source data to data storage destination.\n",
    "\n",
    "`from butterfree.core.pipelines import ...`\n",
    "\n",
    "* `feature_set_pipeline`: defines an ETL pipeline for creating feature sets.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example:\n",
    "Simulating the following scenario:\n",
    "\n",
    "- We want to create a feature set with features about houses for rent (listings).\n",
    "\n",
    "- We are interested in houses only for the **Kanto** region.\n",
    "\n",
    "We have two sets of data:\n",
    "\n",
    "- Table: `listing_events`. Table with data about events of house listings.\n",
    "- File: `region.json`. Static file with data about the cities and regions.\n",
    "\n",
    "Our desire is to have result dataset with the following schema:\n",
    "\n",
    "| id | timestamp | rent | rent_over_area | bedrooms | bathrooms | area | bedrooms_over_area | bathrooms_over_area | latitude | longitude | h3 | city | region \n",
    "| - | - | - | - | - | - | - | - | - | - | - | - | - | - |\n",
    "| int | timestamp | float | float | int | int | float | float | float | double | double | string | string | string |\n",
    "\n",
    "For more information about H3 geohash click [here]()\n",
    "\n",
    "The following code blocks will show how to generate this feature set using Butterfree library:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup spark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import session\n",
    "\n",
    "conf = SparkConf().set('spark.driver.host','127.0.0.1')\n",
    "sc = SparkContext(conf=conf)\n",
    "spark = session.SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix working dir\n",
    "import pathlib\n",
    "import os\n",
    "path = os.path.join(pathlib.Path().absolute(), '../..')\n",
    "os.chdir(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Showing test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> listing_events table:\n",
      "+----+---------+--------+---+---------+----+-------------+\n",
      "|area|bathrooms|bedrooms| id|region_id|rent|    timestamp|\n",
      "+----+---------+--------+---+---------+----+-------------+\n",
      "|  50|        1|       1|  1|        1|1300|1588302000000|\n",
      "|  50|        1|       1|  1|        1|2000|1588647600000|\n",
      "| 100|        1|       2|  2|        2|1500|1588734000000|\n",
      "| 100|        1|       2|  2|        2|2500|1589252400000|\n",
      "| 150|        2|       2|  3|        3|3000|1589943600000|\n",
      "| 175|        2|       2|  4|        4|3200|1589943600000|\n",
      "| 250|        3|       3|  5|        5|3200|1590030000000|\n",
      "| 225|        3|       2|  6|        6|3200|1590116400000|\n",
      "+----+---------+--------+---+---------+----+-------------+\n",
      "\n",
      ">>> region.json file:\n",
      "+--------+---+---------+----------+------+\n",
      "|    city| id|      lat|       lng|region|\n",
      "+--------+---+---------+----------+------+\n",
      "|Cerulean|  1| 73.44489|   31.7503| Kanto|\n",
      "|Veridian|  2|  -9.4351|-167.11772| Kanto|\n",
      "|Cinnabar|  3| 29.73043| 117.66164| Kanto|\n",
      "|  Pallet|  4|-52.95717| -81.15251| Kanto|\n",
      "|  Violet|  5|-47.35798|-178.77255| Johto|\n",
      "| Olivine|  6|  51.7282|  46.21958| Johto|\n",
      "+--------+---+---------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "listing_evengs_df = spark.read.json(\"listing_events.json\")\n",
    "listing_evengs_df.write.mode(\"overwrite\").saveAsTable(\"listing_events\")  # creating listing_events table\n",
    "\n",
    "print(\">>> listing_events table:\")\n",
    "listing_evengs_df.show()\n",
    "\n",
    "print(\">>> region.json file:\")\n",
    "spark.read.json(\"region.json\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract\n",
    "\n",
    "- For the extract part, we need the `Source` entity and the `FileReader` and `TableReader` for the data we have.\n",
    "- We need to declare a query with the rule for joining the results of the readers too.\n",
    "- As proposed in the problem we can filter the region dataset to get only **Kanto** region.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from butterfree.core.extract import Source\n",
    "from butterfree.core.extract.readers import FileReader, TableReader\n",
    "from butterfree.core.extract.pre_processing import filter\n",
    "\n",
    "readers = [\n",
    "    TableReader(id=\"listing_events\", database=\"default\", table=\"listing_events\",),\n",
    "    FileReader(id=\"region\", path=\"region.json\", format=\"json\",).with_(\n",
    "        transformer=filter, condition=\"region == 'Kanto'\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "query = \"\"\"\n",
    "select\n",
    "    listing_events.*,\n",
    "    region.city,\n",
    "    region.region,\n",
    "    region.lat,\n",
    "    region.lng,\n",
    "    region.region as region_name\n",
    "from\n",
    "    listing_events\n",
    "    join region\n",
    "      on listing_events.region_id = region.id\n",
    "\"\"\"\n",
    "\n",
    "source = Source(readers=readers, query=query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform\n",
    "- At the transform part, a set of `Feature` objects is declared.\n",
    "- An Instance of `FeatureSet` is used to hold the features.\n",
    "- A `FeatureSet` can only be created when it is possible to define a unique tuple formed by key columns and a time reference. This is an **architectural requirement** for the data. So least one `KeyFeature` and one `TimestampFeature` is needed.\n",
    "- Every `Feature` needs a unique name, a description, and a data-type definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from butterfree.core.transform import FeatureSet\n",
    "from butterfree.core.transform.features import Feature, KeyFeature, TimestampFeature\n",
    "from butterfree.core.transform.transformations import SQLExpressionTransform\n",
    "from butterfree.core.transform.transformations.h3_transform import H3HashTransform\n",
    "from butterfree.core.constants.data_type import DataType\n",
    "\n",
    "keys = [\n",
    "    KeyFeature(\n",
    "        name=\"id\",\n",
    "        description=\"Unique identificator code for houses.\",\n",
    "        dtype=DataType.BIGINT,\n",
    "    )\n",
    "]\n",
    "\n",
    "# from_ms = True because the data originally is not in a Timestamp format.\n",
    "ts_feature = TimestampFeature(from_column=\"timestamp\", from_ms=True)\n",
    "\n",
    "features = [\n",
    "    Feature(\n",
    "        name=\"rent\",\n",
    "        description=\"Rent value by month described in the listing.\",\n",
    "        dtype=DataType.FLOAT,\n",
    "    ),\n",
    "    Feature(\n",
    "        name=\"rent_over_area\",\n",
    "        description=\"Rent value by month divided by the area of the house.\",\n",
    "        transformation=SQLExpressionTransform(\"rent / area\"),\n",
    "        dtype=DataType.FLOAT,\n",
    "    ),\n",
    "    Feature(\n",
    "        name=\"bedrooms\",\n",
    "        description=\"Number of bedrooms of the house.\",\n",
    "        dtype=DataType.INTEGER,\n",
    "    ),\n",
    "    Feature(\n",
    "        name=\"bathrooms\",\n",
    "        description=\"Number of bathrooms of the house.\",\n",
    "        dtype=DataType.INTEGER,\n",
    "    ),\n",
    "    Feature(\n",
    "        name=\"area\",\n",
    "        description=\"Area of the house, in squared meters.\",\n",
    "        dtype=DataType.STRING,\n",
    "    ),\n",
    "    Feature(\n",
    "        name=\"bedrooms_over_area\",\n",
    "        description=\"Number of bedrooms divided by the area.\",\n",
    "        transformation=SQLExpressionTransform(\"bedrooms / area\"),\n",
    "        dtype=DataType.STRING,\n",
    "    ),\n",
    "    Feature(\n",
    "        name=\"bathrooms_over_area\",\n",
    "        description=\"Number of bathrooms divided by the area.\",\n",
    "        transformation=SQLExpressionTransform(\"bathrooms / area\"),\n",
    "        dtype=DataType.STRING,\n",
    "    ),\n",
    "    Feature(\n",
    "        name=\"latitude\",\n",
    "        description=\"House location latitude.\",\n",
    "        from_column=\"lat\",  # arg from_column is needed when changing column name\n",
    "        dtype=DataType.DOUBLE,\n",
    "    ),\n",
    "    Feature(\n",
    "        name=\"longitude\",\n",
    "        description=\"House location longitude.\",\n",
    "        from_column=\"lng\",\n",
    "        dtype=DataType.DOUBLE,\n",
    "    ),\n",
    "    Feature(\n",
    "        name=\"h3\",\n",
    "        description=\"H3 hash geohash.\",\n",
    "        transformation=H3HashTransform(\n",
    "            h3_resolutions=[10], lat_column=\"latitude\", lng_column=\"longitude\",\n",
    "        ),\n",
    "        dtype=DataType.STRING,\n",
    "    ),\n",
    "    Feature(name=\"city\", description=\"House location city.\", dtype=DataType.STRING,),\n",
    "    Feature(\n",
    "        name=\"region\",\n",
    "        description=\"House location region.\",\n",
    "        from_column=\"region_name\",\n",
    "        dtype=DataType.STRING,\n",
    "    ),\n",
    "]\n",
    "\n",
    "feature_set = FeatureSet(\n",
    "    name=\"house_listings\",\n",
    "    entity=\"house\",  # entity: to which \"business context\" this feature set belongs\n",
    "    description=\"Features describring a house listing.\",\n",
    "    keys=keys,\n",
    "    timestamp=ts_feature,\n",
    "    features=features,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load\n",
    "\n",
    "- For the load part we need `Writer` instances and a `Sink`.\n",
    "- writers define where to load the data.\n",
    "- The `Sink` gets the transformed data (feature set) and trigger the load to all the defined writers.\n",
    "- `debug_mode` will create a temporary view instead of trying to write in a real data store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from butterfree.core.load.writers import (\n",
    "    HistoricalFeatureStoreWriter,\n",
    "    OnlineFeatureStoreWriter,\n",
    ")\n",
    "from butterfree.core.load import Sink\n",
    "\n",
    "writers = [HistoricalFeatureStoreWriter(debug_mode=True), OnlineFeatureStoreWriter(debug_mode=True)]\n",
    "sink = Sink(writers=writers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline\n",
    "\n",
    "- The `Pipeline` entity wraps all the other defined elements.\n",
    "- `run` command will trigger the execution of the pipeline, end-to-end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from butterfree.core.pipelines import FeatureSetPipeline\n",
    "\n",
    "pipeline = FeatureSetPipeline(source=source, feature_set=feature_set, sink=sink)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rafaelpereira/butterfree/butterfree/core/transform/features/feature.py:87: UserWarning: The column name timestamp already exists in the dataframe and will be overwritten with another column.\n",
      "  f\"The column name {self.name} \"\n",
      "/home/rafaelpereira/butterfree/butterfree/core/transform/features/feature.py:87: UserWarning: The column name region already exists in the dataframe and will be overwritten with another column.\n",
      "  f\"The column name {self.name} \"\n"
     ]
    }
   ],
   "source": [
    "result_df = pipeline.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Showing the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>div.output_area pre {white-space: pre;}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Historical Feature house_listings feature set table:\n",
      "+---+-------------------+------+------------------+--------+---------+----+--------------------+--------------------+---------+----------+--------------------+--------+------+----+-----+---+\n",
      "| id|          timestamp|  rent|    rent_over_area|bedrooms|bathrooms|area|  bedrooms_over_area| bathrooms_over_area| latitude| longitude|lat_lng__h3_hash__10|    city|region|year|month|day|\n",
      "+---+-------------------+------+------------------+--------+---------+----+--------------------+--------------------+---------+----------+--------------------+--------+------+----+-----+---+\n",
      "|  1|2020-05-01 00:00:00|1300.0|              26.0|       1|        1|  50|                0.02|                0.02| 73.44489|   31.7503|     8a011c942b5ffff|Cerulean| Kanto|2020|    5|  1|\n",
      "|  1|2020-05-05 00:00:00|2000.0|              40.0|       1|        1|  50|                0.02|                0.02| 73.44489|   31.7503|     8a011c942b5ffff|Cerulean| Kanto|2020|    5|  5|\n",
      "|  2|2020-05-06 00:00:00|1500.0|              15.0|       2|        1| 100|                0.02|                0.01|  -9.4351|-167.11772|     8a9a807200f7fff|Veridian| Kanto|2020|    5|  6|\n",
      "|  2|2020-05-12 00:00:00|2500.0|              25.0|       2|        1| 100|                0.02|                0.01|  -9.4351|-167.11772|     8a9a807200f7fff|Veridian| Kanto|2020|    5| 12|\n",
      "|  3|2020-05-20 00:00:00|3000.0|              20.0|       2|        2| 150|0.013333333333333334|0.013333333333333334| 29.73043| 117.66164|     8a419174230ffff|Cinnabar| Kanto|2020|    5| 20|\n",
      "|  4|2020-05-20 00:00:00|3200.0|18.285714285714285|       2|        2| 175|0.011428571428571429|0.011428571428571429|-52.95717| -81.15251|     8acf2ab9d74ffff|  Pallet| Kanto|2020|    5| 20|\n",
      "+---+-------------------+------+------------------+--------+---------+----+--------------------+--------------------+---------+----------+--------------------+--------+------+----+-----+---+\n",
      "\n",
      ">>> Online Feature house_listings feature set table:\n",
      "+---+-------------------+------+------------------+--------+---------+----+--------------------+--------------------+---------+----------+--------------------+--------+------+\n",
      "| id|          timestamp|  rent|    rent_over_area|bedrooms|bathrooms|area|  bedrooms_over_area| bathrooms_over_area| latitude| longitude|lat_lng__h3_hash__10|    city|region|\n",
      "+---+-------------------+------+------------------+--------+---------+----+--------------------+--------------------+---------+----------+--------------------+--------+------+\n",
      "|  1|2020-05-05 00:00:00|2000.0|              40.0|       1|        1|  50|                0.02|                0.02| 73.44489|   31.7503|     8a011c942b5ffff|Cerulean| Kanto|\n",
      "|  2|2020-05-12 00:00:00|2500.0|              25.0|       2|        1| 100|                0.02|                0.01|  -9.4351|-167.11772|     8a9a807200f7fff|Veridian| Kanto|\n",
      "|  3|2020-05-20 00:00:00|3000.0|              20.0|       2|        2| 150|0.013333333333333334|0.013333333333333334| 29.73043| 117.66164|     8a419174230ffff|Cinnabar| Kanto|\n",
      "|  4|2020-05-20 00:00:00|3200.0|18.285714285714285|       2|        2| 175|0.011428571428571429|0.011428571428571429|-52.95717| -81.15251|     8acf2ab9d74ffff|  Pallet| Kanto|\n",
      "+---+-------------------+------+------------------+--------+---------+----+--------------------+--------------------+---------+----------+--------------------+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# fixing wrap output\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>div.output_area pre {white-space: pre;}</style>\"))\n",
    "\n",
    "print(\">>> Historical Feature house_listings feature set table:\")\n",
    "spark.table(\"historical_feature_store__house_listings\").orderBy(\n",
    "    \"id\", \"timestamp\"\n",
    ").show()\n",
    "\n",
    "print(\">>> Online Feature house_listings feature set table:\")\n",
    "spark.table(\"online_feature_store__house_listings\").orderBy(\"id\", \"timestamp\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can see that we were able to create all the desired features in an easy way\n",
    "- The **historical feature set** holds all the data, and we can see that it is partitioned by year, month and day (columns added in the `HistoricalFeatureStoreWriter`)\n",
    "- In the **online feature set** there is only the latest data for each id"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
