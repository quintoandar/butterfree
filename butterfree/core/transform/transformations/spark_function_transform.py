"""Spark Function Transform entity."""
from typing import List

from pyspark.sql.functions import lit

from butterfree.core.constants.aggregated_type import ALLOWED_AGGREGATIONS
from butterfree.core.transform.transformations.transform_component import (
    TransformComponent,
)


class SparkFunctionTransform(TransformComponent):
    """Defines an Spark Function.

    Attributes:
        functions: spark functions to be used, it can be avg, std, count and others.

    Example:
        It's necessary to declare the function method, (average,
        standard deviation and count are currently supported).
        >>> from butterfree.core.transform.transformations import SparkFunctionTransform
        >>> from butterfree.core.transform.features import Feature
        >>> from pyspark import SparkContext
        >>> from pyspark.sql import session
        >>> from pyspark.sql.types import TimestampType
        >>> sc = SparkContext.getOrCreate()
        >>> spark = session.SparkSession(sc)
        >>> df = spark.createDataFrame([(1, "2016-04-11 11:31:11", 200),
        ...                             (1, "2016-04-11 11:44:12", 300),
        ...                             (1, "2016-04-11 11:46:24", 400),
        ...                             (1, "2016-04-11 12:03:21", 500)]
        ...                           ).toDF("id", "timestamp", "feature")
        >>> df = df.withColumn("timestamp", df.timestamp.cast(TimestampType()))
        >>> feature = Feature(
        ...    name="feature",
        ...    description="spark function transform",
        ...    transformation=SparkFunctionTransform(functions=["avg"],)
        ...)
        >>> feature.transform(df).orderBy("timestamp").show()
        +--------+-----------------------+------------+
        |feature | id|          timestamp| feature_avg|
        +--------+---+-------------------+------------+
        |     200|  1|2016-04-11 11:31:11|       350.0|
        |     300|  1|2016-04-11 11:44:12|       350.0|
        |     400|  1|2016-04-11 11:46:24|       350.0|
        |     500|  1|2016-04-11 12:03:21|       350.0|
        +--------+---+-------------------+------------+

        We can use this transformation with windows.
        >>> feature_rolling_windows = Feature(
        ...    name="feature",
        ...    description="spark function transform with windows",
        ...    transformation=SparkFunctionTransform(functions=["avg"],).with_(
        ...        function=with_window,
        ...        partition_by="id",
        ...        order_by=TIMESTAMP_COLUMN,
        ...        mode="row_windows",
        ...        window_definition=["2 events"],
        ...   )
        ...)
        >>> feature_rolling_windows.transform(df).orderBy("timestamp").show()
        +--------+-----------------------+---------------------------------------+
        |feature | id|          timestamp| feature__avg_over_2_events_row_windows|
        +--------+---+-------------------+---------------------------------------+
        |     200|  1|2016-04-11 11:31:11|                                  200.0|
        |     300|  1|2016-04-11 11:44:12|                                  250.0|
        |     400|  1|2016-04-11 11:46:24|                                  350.0|
        |     500|  1|2016-04-11 12:03:21|                                  450.0|
        +--------+---+-------------------+---------------------------------------+

        It's important to notice that transformation doesn't affect the
        dataframe granularity.

    """

    def __init__(self, functions):
        super().__init__()
        self.functions = functions

    @property
    def functions(self) -> List[str]:
        """Functions to be used in this transformation."""
        return self._functions

    @functions.setter
    def functions(self, value: List[str]):
        """Functions definitions to be used in this transformation."""
        functions = []
        if not value:
            raise ValueError("Functions must not be empty.")
        for f in value:
            if f not in ALLOWED_AGGREGATIONS:
                raise KeyError(
                    f"{f} is not supported. These are the allowed "
                    f"functions that you can use: "
                    f"{ALLOWED_AGGREGATIONS}"
                )
            functions.append(f)
        self._functions = functions

    def _get_name(self, function, window=None):
        if window:
            return "_".join([self._parent.name, function, window.get_name()])

        return "_".join([self._parent.name, function])

    @property
    def output_columns(self) -> List[str]:
        """Columns generated by the transformation."""
        output_columns = []
        for function in self.functions:
            for window in self._windows:
                output_columns.append(self._get_name(function, window))

        return output_columns

    def transform(self, dataframe):
        """Performs a transformation to the feature pipeline.

        Args:
            dataframe: input dataframe.

        Returns:
            Transformed dataframe.
        """
        for f in self.functions:
            if self._windows:
                for window in self._windows:
                    dataframe = dataframe.withColumn(
                        self._get_name(f, window),
                        ALLOWED_AGGREGATIONS[f](
                            self._parent.from_column or self._parent.name
                        ).over(window.get()),
                    )
            else:
                dataframe = dataframe.withColumn(
                    self._get_name(f),
                    lit(
                        dataframe.select(
                            ALLOWED_AGGREGATIONS[f](
                                self._parent.from_column or self._parent.name
                            )
                        ).first()[0]
                    ),
                )
        return dataframe
