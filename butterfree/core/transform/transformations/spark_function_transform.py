"""Spark Function Transform entity."""
from typing import List

from butterfree.core.transform.transformations.transform_component import (
    TransformComponent,
)
from butterfree.core.transform.utils import Window


class SparkFunctionTransform(TransformComponent):
    """Defines an Spark Function.

    Attributes:
        function: spark functions to be used.
         For more information check functions:
         'https://spark.apache.org/docs/2.3.1/api/python/_modules/pyspark/sql/functions.html'.

    Example:
        It's necessary to declare the function method, (average,
        standard deviation and count are currently supported).
        >>> from butterfree.core.transform.transformations import SparkFunctionTransform
        >>> from butterfree.core.constants.columns import TIMESTAMP_COLUMN
        >>> from butterfree.core.transform.features import Feature
        >>> from pyspark import SparkContext
        >>> from pyspark.sql import session
        >>> from pyspark.sql.types import TimestampType
        >>> from pyspark.sql import functions
        >>> sc = SparkContext.getOrCreate()
        >>> spark = session.SparkSession(sc)
        >>> df = spark.createDataFrame([(1, "2016-04-11 11:31:11", 200),
        ...                             (1, "2016-04-11 11:44:12", 300),
        ...                             (1, "2016-04-11 11:46:24", 400),
        ...                             (1, "2016-04-11 12:03:21", 500)]
        ...                           ).toDF("id", "timestamp", "feature")
        >>> df = df.withColumn("timestamp", df.timestamp.cast(TimestampType()))
        >>> feature = Feature(
        ...    name="feature",
        ...    description="spark function transform",
        ...    transformation=SparkFunctionTransform(function=functions.ceil,)
        ...)
        >>> feature.transform(df).orderBy("timestamp").show()
        +---+-------------------+-------+------------+
        | id|          timestamp|feature|feature_ceil|
        +---+-------------------+-------+------------+
        |  1|2016-04-11 11:31:11|    200|         200|
        |  1|2016-04-11 11:44:12|    300|         300|
        |  1|2016-04-11 11:46:24|    400|         400|
        |  1|2016-04-11 12:03:21|    500|         500|
        +---+-------------------+-------+------------+


        We can use this transformation with windows.
        >>> feature_row_windows = Feature(
        ...    name="feature",
        ...    description="spark function transform with windows",
        ...    transformation=SparkFunctionTransform(functions.avg,).with_window(
        ...        partition_by="id",
        ...        order_by=TIMESTAMP_COLUMN,
        ...        mode="row_windows",
        ...        window_definition=["2 events"],
        ...   )
        ...)
        >>> feature_row_windows.transform(df).orderBy("timestamp").show()
        +--------+-----------------------+---------------------------------------+
        |feature | id|          timestamp| feature_avg_over_2_events_row_windows|
        +--------+---+-------------------+--------------------------------------+
        |     200|  1|2016-04-11 11:31:11|                                 200.0|
        |     300|  1|2016-04-11 11:44:12|                                 250.0|
        |     400|  1|2016-04-11 11:46:24|                                 350.0|
        |     500|  1|2016-04-11 12:03:21|                                 450.0|
        +--------+---+-------------------+--------------------------------------+

        It's important to notice that transformation doesn't affect the
        dataframe granularity.

    """

    def __init__(self, function):
        super().__init__()
        self.function = function
        self._windows = []

    def with_window(self, partition_by, order_by, mode=None, window_definition=None):
        """Create a list with windows defined."""
        windows = []
        if mode is not None:
            for definition in window_definition:
                windows.append(Window(partition_by, order_by, mode, definition))

        self._windows = windows
        return self

    def _get_name_with_window(self, window):
        if hasattr(self.function, "__name__"):
            return "_".join(
                [self._parent.name, self.function.__name__, window.get_name()]
            )

        return "_".join([self._parent.name, window.get_name()])

    @property
    def output_columns(self) -> List[str]:
        """Columns generated by the transformation."""
        name = self._parent.name
        output_columns = []
        for window in self._windows:
            output_columns.append(self._get_name_with_window(window))

        return output_columns or [name]

    def transform(self, dataframe):
        """Performs a transformation to the feature pipeline.

        Args:
            dataframe: input dataframe.

        Returns:
            Transformed dataframe.
        """
        if self._windows:
            for window in self._windows:
                dataframe = dataframe.withColumn(
                    self._get_name_with_window(window),
                    self.function(self._parent.from_column or self._parent.name).over(
                        window.get()
                    ),
                )
            return dataframe

        return dataframe.withColumn(
            self._parent.name,
            self.function(self._parent.from_column or self._parent.name),
        )
